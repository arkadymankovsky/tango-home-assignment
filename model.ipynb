{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T13:08:47.197390Z",
     "start_time": "2025-02-24T13:08:32.130899Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T13:08:47.210084Z",
     "start_time": "2025-02-24T13:08:47.200401Z"
    }
   },
   "source": [
    "def load_data():\n",
    "    dataset_file = 'dataset_purchases.csv'\n",
    "    df = pd.read_csv(dataset_file)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['usd'] = df['usd'].round(2)\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['week'] = df['timestamp'].dt.isocalendar().week\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T13:40:47.443256Z",
     "start_time": "2025-02-24T13:40:47.430091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_data(df):\n",
    "    \n",
    "    # Create item_id from price-coins combination\n",
    "    df['item_id'] = df['usd'].astype(str) + '_' + df['coins'].astype(str)\n",
    "    df['user_hour'] = df['user_id'].astype(str) + '_' + df['hour'].astype(str)\n",
    "    \n",
    "    # Calculate purchase frequency and normalize VFM\n",
    "    purchase_frequency = df.groupby(['user_hour', 'item_id']).size().reset_index(name='frequency')\n",
    "    avg_vfm = df.groupby(['item_id'])['VFM'].mean().reset_index(name='avg_vfm')\n",
    "    \n",
    "    # Normalize frequency and VFM to 0-1 scale\n",
    "    purchase_frequency['norm_frequency'] = purchase_frequency['frequency'] / purchase_frequency['frequency'].max()\n",
    "    avg_vfm['norm_vfm'] = (avg_vfm['avg_vfm'] - avg_vfm['avg_vfm'].min()) / (avg_vfm['avg_vfm'].max() - avg_vfm['avg_vfm'].min())\n",
    "    \n",
    "    # Combine frequency and VFM into final rating\n",
    "    df = df.merge(purchase_frequency[['user_hour', 'item_id', 'norm_frequency']], on=['user_hour', 'item_id'])\n",
    "    df = df.merge(avg_vfm[['item_id', 'norm_vfm']], on='item_id')\n",
    "    \n",
    "    # Create actual rating based on frequency and VFM (you can adjust the weighting)\n",
    "    df['rating'] = 0.7 * df['norm_frequency'] + 0.3 * df['norm_vfm']\n",
    "    \n",
    "    # Split the data\n",
    "    train_df, test_df = train_test_split(\n",
    "        df,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=df['user_id']\n",
    "    )\n",
    "    \n",
    "    # Create Surprise dataset - FIXED HERE: only pass 3 columns\n",
    "    reader = Reader(rating_scale=(0, 1))\n",
    "    trainset = Dataset.load_from_df(\n",
    "        train_df[['user_hour', 'item_id', 'rating']], \n",
    "        reader\n",
    "    ).build_full_trainset()\n",
    "    \n",
    "    # Add analysis of the ratings\n",
    "    print(\"\\nRating Statistics:\")\n",
    "    print(f\"Average rating: {df['rating'].mean():.3f}\")\n",
    "    print(f\"Rating standard deviation: {df['rating'].std():.3f}\")\n",
    "    print(\"\\nCorrelations with rating:\")\n",
    "    print(f\"Frequency correlation: {df['norm_frequency'].corr(df['rating']):.3f}\")\n",
    "    print(f\"VFM correlation: {df['norm_vfm'].corr(df['rating']):.3f}\")\n",
    "    \n",
    "    return trainset, train_df, test_df"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T13:40:47.835196Z",
     "start_time": "2025-02-24T13:40:47.826860Z"
    }
   },
   "source": [
    "def train_model(trainset):\n",
    "    \"\"\"\n",
    "    Train SVD model with optimized default parameters for the enriched rating system\n",
    "    \"\"\"\n",
    "    # Initialize model with carefully chosen parameters\n",
    "    model = SVD(\n",
    "        n_factors=50,     # More factors to capture VFM patterns\n",
    "        n_epochs=30,      # More epochs for better convergence\n",
    "        lr_all=0.1,     # Slightly lower learning rate for stability\n",
    "        reg_all=0.05      # Moderate regularization\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(trainset)\n",
    "    return model\n",
    "\n",
    "def get_recommendations(model, train_df, user_id, hour, n_items=6):\n",
    "    user_hour = f\"{user_id}_{hour}\"\n",
    "    items = train_df['item_id'].unique()\n",
    "    predictions = [model.predict(user_hour, item_id) for item_id in items]\n",
    "    top_items = sorted(predictions, key=lambda x: x.est, reverse=True)[:n_items]\n",
    "    return [pred.iid for pred in top_items]"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T13:41:29.733052Z",
     "start_time": "2025-02-24T13:40:48.476542Z"
    }
   },
   "source": [
    "df = load_data()\n",
    "trainset, train_df, test_df = prepare_data(df)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rating Statistics:\n",
      "Average rating: 0.065\n",
      "Rating standard deviation: 0.057\n",
      "\n",
      "Correlations with rating:\n",
      "Frequency correlation: 0.359\n",
      "VFM correlation: 0.858\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T13:42:14.676244Z",
     "start_time": "2025-02-24T13:41:29.737066Z"
    }
   },
   "source": [
    "model = train_model(trainset)"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T13:44:12.900848Z",
     "start_time": "2025-02-24T13:42:14.681490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "sample_size = 200\n",
    "hit_rate_list = []\n",
    "for i in range(2):\n",
    "    hit_rate = 0\n",
    "    for index, row  in test_df.sample(sample_size).iterrows():\n",
    "        user_id = row['user_id']\n",
    "        hour = row['hour']\n",
    "        actual_item = row['item_id']\n",
    "        predictions = get_recommendations(model, train_df, user_id, hour, 6)\n",
    "        if actual_item in predictions:\n",
    "            hit_rate += 1\n",
    "    hit_rate_list.append(hit_rate / sample_size)\n",
    "print('Avg Hit Rate:', np.mean(hit_rate_list))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Hit Rate: 0.0075\n",
      "CPU times: total: 55.4 s\n",
      "Wall time: 1min 58s\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tango_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
